{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78142b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "myConf=pyspark.SparkConf()\n",
    "\n",
    "spark= pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master('local')\\\n",
    "    .appName('myApp')\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2680d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[('1', 'kim, js', 170),\n",
    "       ('1', 'lee, sm', 175),\n",
    "       ('2', 'lim, yg', 180),\n",
    "       ('2', 'lee', 170)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a889b87",
   "metadata": {},
   "source": [
    "# DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e17308",
   "metadata": {},
   "source": [
    "### List로 Dataframe 생성(schema 설정X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a93880e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf= spark.createDataFrame(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a63aca20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_1', '_2', '_3']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "579b5db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2018811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(_1='1', _2='kim, js', _3=170)]\n"
     ]
    }
   ],
   "source": [
    "print(myDf.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a6fe24",
   "metadata": {},
   "source": [
    "### List로 Dataframe 생성(schema 설정O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32705f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols= ['year', 'name', 'height']\n",
    "_myDf= spark.createDataFrame(myList, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec430f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year', 'name', 'height']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b9210ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6276e776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(year='1', name='kim, js', height=170)]\n"
     ]
    }
   ],
   "source": [
    "print(_myDf.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5f2b22",
   "metadata": {},
   "source": [
    "### 임의의 데이터 100개로 Dataframe 생성(schema 설정O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8de8fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "names=['kim', 'lee', 'lee', 'lim']\n",
    "items=['espresso', 'latte', 'americano', 'affocato', 'long black', 'macciato']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac5d737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffeeDf= spark.createDataFrame([(names[i%4], items[i%6]) for i in range(100)],\\\n",
    "                              [\"name\", \"coffee\"]) #column= [\"name\", \"coffee\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b3d79d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- coffee: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coffeeDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8abdd74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|name|    coffee|\n",
      "+----+----------+\n",
      "| kim|  espresso|\n",
      "| lee|     latte|\n",
      "| lee| americano|\n",
      "| lim|  affocato|\n",
      "| kim|long black|\n",
      "| lee|  macciato|\n",
      "| lee|  espresso|\n",
      "| lim|     latte|\n",
      "| kim| americano|\n",
      "| lee|  affocato|\n",
      "+----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coffeeDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a82cba",
   "metadata": {},
   "source": [
    "### Row 객체를 이용하여 Dictionary/DataFrame 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05f617d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "person= Row('year', 'name', 'height') \n",
    "row1= person('1', 'kim, js', 170) #열 이름이 person 리스트로 들어감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f88f99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row1:  1 kim, js 170\n",
      "<class 'pyspark.sql.types.Row'>\n"
     ]
    }
   ],
   "source": [
    "print(\"row1: \", row1.year, row1.name, row1.height)\n",
    "print(type(row1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b2ae49",
   "metadata": {},
   "source": [
    "Dictionary 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b50fd4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'year': '1', 'name': 'kim, js', 'height': 170}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row1.asDict() # row -> dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec34484d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['year', 'name', 'height'])\n",
      "dict_values(['1', 'kim, js', 170])\n"
     ]
    }
   ],
   "source": [
    "print(row1.asDict().keys())\n",
    "print(row1.asDict().values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e97f49",
   "metadata": {},
   "source": [
    "DataFrame 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5697e926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(year='1', name='kim, js', height=170), Row(year='1', name='lee, sm', height=175), Row(year='2', name='lim, yg', height=180), Row(year='2', name='lee', height=170)]\n"
     ]
    }
   ],
   "source": [
    "myRows=[row1,\n",
    "       person('1', 'lee, sm', 175),\n",
    "       person('2', 'lim, yg', 180),\n",
    "       person('2', 'lee', 170)]\n",
    "print(myRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "274993b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf= spark.createDataFrame(myRows)\n",
    "# 모든 튜플에 person을 씌운 덕분에 myRows의 column명이 person으로 설정됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c49259fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      "\n",
      "None\n",
      "+----+-------+------+\n",
      "|year|   name|height|\n",
      "+----+-------+------+\n",
      "|   1|kim, js|   170|\n",
      "|   1|lee, sm|   175|\n",
      "|   2|lim, yg|   180|\n",
      "|   2|    lee|   170|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(myDf.printSchema())\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1906484d",
   "metadata": {},
   "source": [
    "### schema 정의, 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41608158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "mySchema=StructType([ #구조체 선언\n",
    "    StructField(\"year\", StringType(), True), #column명, dataType, Null 허용여부\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", IntegerType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dcc23ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf= spark.createDataFrame(myRows, mySchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b27e8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92400c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+\n",
      "|year|   name|height|\n",
      "+----+-------+------+\n",
      "|   1|kim, js|   170|\n",
      "|   1|lee, sm|   175|\n",
      "|   2|lim, yg|   180|\n",
      "|   2|    lee|   170|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9aef24",
   "metadata": {},
   "source": [
    "### RDD -> DF (Spark가 schema를 유추하게 됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a23f575",
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[('1', 'kim, js', 170),\n",
    "       ('1', 'lee, sm', 175),\n",
    "       ('2', 'lim, yg', 180),\n",
    "       ('2', 'lee', 170)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "202416bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd= spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "059fab29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf= myRdd.toDF() # rdd -> DF 1번째 방법 \n",
    "rddDf.printSchema() #schema는 자동으로 삽입됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "978bcf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_rddDf= spark.createDataFrame(myRdd) # rdd -> DF 2번째 방법\n",
    "_rddDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6cf277",
   "metadata": {},
   "source": [
    "### Row를 사용하여 Rdd 형변환 및 column 설정, 이후 rdd ->Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9bcd3032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 'kim, js', 170),\n",
       " ('1', 'lee, sm', 175),\n",
       " ('2', 'lim, yg', 180),\n",
       " ('2', 'lee', 170)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27129cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "testRdd= myRdd.map(lambda x:Row(year=int(x[0]), name=x[1], height=int(x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "678c1fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF= spark.createDataFrame(testRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2b2f62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- height: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d6413a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----+\n",
      "|height|   name|year|\n",
      "+------+-------+----+\n",
      "|   170|kim, js|   1|\n",
      "|   175|lee, sm|   1|\n",
      "|   180|lim, yg|   2|\n",
      "|   170|    lee|   2|\n",
      "+------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257fbe57",
   "metadata": {},
   "source": [
    "### Row를 사용하여 rdd 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1827df3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "r1= Row(name='js1', age=10)\n",
    "r2= Row(name='js2', age=20)\n",
    "myRdd= spark.sparkContext.parallelize([r1,r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48bb6426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=10, name='js1'), Row(age=20, name='js2')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1dab19",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef5c552",
   "metadata": {},
   "source": [
    "### DataFrame -> pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d34eddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#myDf= spark.createDataFrame(myRows, mySchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8463eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>kim, js</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>lee, sm</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>lim, yg</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>lee</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  year     name  height\n",
       "0    1  kim, js     170\n",
       "1    1  lee, sm     175\n",
       "2    2  lim, yg     180\n",
       "3    2      lee     170"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b95afd5",
   "metadata": {},
   "source": [
    "### DF를 write.format()/pandas를 이용하여 csv로 내보내기 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b66f9a",
   "metadata": {},
   "source": [
    "write.format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a7d4752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# DF를 write.format('com.databricks.spark.csv')를 이용하여 csv로 내보내기\n",
    "myDf.write.format('com.databricks.spark.csv').save(os.path.join('data', '_myDf.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6be2ec41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-24a116be-9f80-48e7-bb03-e5d548fde0fa-c000.csv  _SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "!dir data/_myDf.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b4d2d7",
   "metadata": {},
   "source": [
    "pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9281d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF를 pandas를 이용하여 csv로 내보내기\n",
    "myDf.toPandas().to_csv(os.path.join(\"data\", 'pandas_myDf.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b0a5057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/pandas_myDf.csv\r\n"
     ]
    }
   ],
   "source": [
    "!dir data/pandas_myDf.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be528ed",
   "metadata": {},
   "source": [
    "### pandas에서 column 생성, 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a1a1bf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "icc= pd.DataFrame({'contry': ['South Korea', 'Japen','Honkong'], 'codes':[81, 82, 83]})\n",
    "#{column:[value]} 형식으로 column 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f474f1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contry</th>\n",
       "      <th>codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>South Korea</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Japen</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Honkong</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        contry  codes\n",
       "0  South Korea     81\n",
       "1        Japen     82\n",
       "2      Honkong     83"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "db4d05cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contry</th>\n",
       "      <th>codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>South Korea</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        contry  codes\n",
       "0  South Korea     81"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icc[icc['codes']==81]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ff2990",
   "metadata": {},
   "source": [
    "### Rdd로 csv 파일을 읽어서 DF로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a145194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/ds_spark_2cols.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_2cols.csv\n",
    "35, 2\n",
    "40, 27\n",
    "12, 38\n",
    "15, 31\n",
    "21, 1\n",
    "14, 19\n",
    "46, 1\n",
    "10, 34\n",
    "28, 3\n",
    "48, 1\n",
    "16, 2\n",
    "30, 3\n",
    "32, 2\n",
    "48, 1\n",
    "31, 2\n",
    "22, 1\n",
    "12, 3\n",
    "39, 29\n",
    "19, 37\n",
    "25, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "36b4bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "cfile= os.path.join('data', 'ds_spark_2cols.csv')\n",
    "lines= spark.sparkContext.textFile(cfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d5802cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_col12= lines.map(lambda l: l.split(\",\"))\n",
    "col12= _col12.map(lambda p: Row(col1= int(p[0].strip()), col2= int(p[1].strip())))\n",
    "\n",
    "rddCsv_myDf= spark.createDataFrame(col12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2f51d234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col1: long (nullable = true)\n",
      " |-- col2: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(col1=35, col2=2),\n",
       " Row(col1=40, col2=27),\n",
       " Row(col1=12, col2=38),\n",
       " Row(col1=15, col2=31),\n",
       " Row(col1=21, col2=1),\n",
       " Row(col1=14, col2=19),\n",
       " Row(col1=46, col2=1),\n",
       " Row(col1=10, col2=34),\n",
       " Row(col1=28, col2=3),\n",
       " Row(col1=48, col2=1),\n",
       " Row(col1=16, col2=2),\n",
       " Row(col1=30, col2=3),\n",
       " Row(col1=32, col2=2),\n",
       " Row(col1=48, col2=1),\n",
       " Row(col1=31, col2=2),\n",
       " Row(col1=22, col2=1),\n",
       " Row(col1=12, col2=3),\n",
       " Row(col1=39, col2=29),\n",
       " Row(col1=19, col2=37),\n",
       " Row(col1=25, col2=2)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddCsv_myDf.printSchema()\n",
    "rddCsv_myDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a85798b",
   "metadata": {},
   "source": [
    "### DF로 csv를 직접 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a07bf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/ds_spark.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark.csv\n",
    "1, 2, 3, 4\n",
    "11, 22, 33, 44\n",
    "111, 222, 333, 444"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aaa266",
   "metadata": {},
   "source": [
    "format().load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fb22aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= spark\\\n",
    "    .read\\\n",
    "    .format('com.databricks.spark.csv')\\\n",
    "    .options(header='true', inferschema=True, delimiter=',')\\\n",
    "    .load(os.path.join('data', 'ds_spark.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b7ec58d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-----+\n",
      "|  1|    2|    3|    4|\n",
      "+---+-----+-----+-----+\n",
      "| 11| 22.0| 33.0| 44.0|\n",
      "|111|222.0|333.0|444.0|\n",
      "+---+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "19f54178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 1: integer (nullable = true)\n",
      " |--  2: double (nullable = true)\n",
      " |--  3: double (nullable = true)\n",
      " |--  4: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b33ec91",
   "metadata": {},
   "source": [
    "csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b8b7f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= spark\\\n",
    "    .read\\\n",
    "    .options(header=True, inferschema=True, delimiter=',')\\\n",
    "    .csv(os.path.join('data', 'ds_spark.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c4598df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-----+\n",
      "|  1|    2|    3|    4|\n",
      "+---+-----+-----+-----+\n",
      "| 11| 22.0| 33.0| 44.0|\n",
      "|111|222.0|333.0|444.0|\n",
      "+---+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2954ab37",
   "metadata": {},
   "source": [
    "### tsv파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e510f6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/ds_spark_heightweight.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_heightweight.txt\n",
    "1\t65.78\t112.99\n",
    "2\t71.52\t136.49\n",
    "3\t69.40\t153.03\n",
    "4\t68.22\t142.34\n",
    "5\t67.79\t144.30\n",
    "6\t68.70\t123.30\n",
    "7\t69.80\t141.49\n",
    "8\t70.01\t136.46\n",
    "9\t67.90\t112.37\n",
    "10\t66.78\t120.67\n",
    "11\t66.49\t127.45\n",
    "12\t67.62\t114.14\n",
    "13\t68.30\t125.61\n",
    "14\t67.12\t122.46\n",
    "15\t68.28\t116.09\n",
    "16\t71.09\t140.00\n",
    "17\t66.46\t129.50\n",
    "18\t68.65\t142.97\n",
    "19\t71.23\t137.90\n",
    "20\t67.13\t124.04\n",
    "21\t67.83\t141.28\n",
    "22\t68.88\t143.54\n",
    "23\t63.48\t97.90\n",
    "24\t68.42\t129.50\n",
    "25\t67.63\t141.85\n",
    "26\t67.21\t129.72\n",
    "27\t70.84\t142.42\n",
    "28\t67.49\t131.55\n",
    "29\t66.53\t108.33\n",
    "30\t65.44\t113.89\n",
    "31\t69.52\t103.30\n",
    "32\t65.81\t120.75\n",
    "33\t67.82\t125.79\n",
    "34\t70.60\t136.22\n",
    "35\t71.80\t140.10\n",
    "36\t69.21\t128.75\n",
    "37\t66.80\t141.80\n",
    "38\t67.66\t121.23\n",
    "39\t67.81\t131.35\n",
    "40\t64.05\t106.71\n",
    "41\t68.57\t124.36\n",
    "42\t65.18\t124.86\n",
    "43\t69.66\t139.67\n",
    "44\t67.97\t137.37\n",
    "45\t65.98\t106.45\n",
    "46\t68.67\t128.76\n",
    "47\t66.88\t145.68\n",
    "48\t67.70\t116.82\n",
    "49\t69.82\t143.62\n",
    "50\t69.09\t134.93"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b6aba2",
   "metadata": {},
   "source": [
    "### tsv를 rdd로 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a2ec4c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "_tRdd= spark.sparkContext.textFile(os.path.join('data', 'ds_spark_heightweight.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "84044986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 65.78, 112.99],\n",
       " [2.0, 71.52, 136.49],\n",
       " [3.0, 69.4, 153.03],\n",
       " [4.0, 68.22, 142.34],\n",
       " [5.0, 67.79, 144.3]]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_tRddSplt=_tRdd.map(lambda line: [float(x) for x in line.split('\\t')])\n",
    "_tRddSplt.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563f0e00",
   "metadata": {},
   "source": [
    "### tsv -> Rdd -> DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "87435f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDF= spark.createDataFrame(_tRddSplt, [\"id\", \"weight\", \"height\"])\n",
    "tDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6aea38aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1.0, weight=65.78, height=112.99),\n",
       " Row(id=2.0, weight=71.52, height=136.49),\n",
       " Row(id=3.0, weight=69.4, height=153.03),\n",
       " Row(id=4.0, weight=68.22, height=142.34),\n",
       " Row(id=5.0, weight=67.79, height=144.3)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tDF.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20477adf",
   "metadata": {},
   "source": [
    "### tsv를 text()로 읽어서 #column=1을 #column=3으로 수정하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0cbc50dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tText= spark.read.text(os.path.join('data', 'ds_spark_heightweight.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "32ad536e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tText.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6a80d9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "split_col= split(tText['value'], '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4e995f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tText= tText.withColumn('weight', split_col.getItem(1))\n",
    "tText= tText.withColumn('height', split_col.getItem(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cb6b7895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+------+\n",
      "|          value|weight|height|\n",
      "+---------------+------+------+\n",
      "| 1\t65.78\t112.99| 65.78|112.99|\n",
      "| 2\t71.52\t136.49| 71.52|136.49|\n",
      "| 3\t69.40\t153.03| 69.40|153.03|\n",
      "| 4\t68.22\t142.34| 68.22|142.34|\n",
      "| 5\t67.79\t144.30| 67.79|144.30|\n",
      "| 6\t68.70\t123.30| 68.70|123.30|\n",
      "| 7\t69.80\t141.49| 69.80|141.49|\n",
      "| 8\t70.01\t136.46| 70.01|136.46|\n",
      "| 9\t67.90\t112.37| 67.90|112.37|\n",
      "|10\t66.78\t120.67| 66.78|120.67|\n",
      "|11\t66.49\t127.45| 66.49|127.45|\n",
      "|12\t67.62\t114.14| 67.62|114.14|\n",
      "|13\t68.30\t125.61| 68.30|125.61|\n",
      "|14\t67.12\t122.46| 67.12|122.46|\n",
      "|15\t68.28\t116.09| 68.28|116.09|\n",
      "|16\t71.09\t140.00| 71.09|140.00|\n",
      "|17\t66.46\t129.50| 66.46|129.50|\n",
      "|18\t68.65\t142.97| 68.65|142.97|\n",
      "|19\t71.23\t137.90| 71.23|137.90|\n",
      "|20\t67.13\t124.04| 67.13|124.04|\n",
      "+---------------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tText.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b577fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
